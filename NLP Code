import nltk
from nltk.tokenize import word_tokenize
from gensim.models import Word2Vec

# Step 1: Data Collection and Preprocessing
# Sample Urdu text data
urdu_text = "آج کل کے دنوں میں سموچے کے ساتھ ٹماٹر کچھ کچھ پڑ گیا ہے۔"

# Tokenize the Urdu text into words
tokens = word_tokenize(urdu_text)

# Step 2: Word Embedding Models
# Train a Word2Vec model on the tokenized Urdu text
model_w2v = Word2Vec([tokens], size=100, window=5, min_count=1)

# Step 3: Unsupervised Morphological Segmentation
# Implement a simple unsupervised morphological segmentation algorithm
def morphological_segmentation(word):
    segments = []
    for i in range(1, len(word) + 1):
        prefix = word[:i]
        suffix = word[i:]
        segments.append((prefix, suffix))
    return segments

# Step 4: Evaluation Metrics
# Calculate the morphological segmentation accuracy
def calculate_accuracy(true_segments, predicted_segments):
    # Check the accuracy of predicted segments against true segments
    correct_segments = set(true_segments) & set(predicted_segments)
    accuracy = len(correct_segments) / len(true_segments)
    return accuracy

# Step 5: Comparative Analysis
# Compare the performance of different word embedding models
word = "سموچے"
true_segments = [('سم', 'وچے'), ('سمو', 'چے'), ('سموچ', 'ے')]
predicted_segments_w2v = morphological_segmentation(word)

# Step 6: Statistical Analysis
# Compare the accuracy of different models
accuracy_w2v = calculate_accuracy(true_segments, predicted_segments_w2v)

# Print the results
print("Word:", word)
print("True Segments:", true_segments)
print("Predicted Segments (Word2Vec):", predicted_segments_w2v)
print("Accuracy (Word2Vec):", accuracy_w2v)
